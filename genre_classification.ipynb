{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "genre_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDAys5NJ9vU7"
      },
      "source": [
        "## Wavelet data generation.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hU730GYrfDj"
      },
      "source": [
        "def func(cls):\n",
        "  img_names = os.listdir('genres/'+cls)\n",
        "  os.makedirs('wavelets/train/'+cls)\n",
        "  os.makedirs('wavelets/test/'+cls)\n",
        "  print(cls)\n",
        "  train_names = img_names[:60]\n",
        "  test_names = img_names[60:]\n",
        "  cnt = 0\n",
        "  for nm in train_names:\n",
        "    cnt+=1\n",
        "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
        "    #plt.figure(figsize=(14, 5))\n",
        "    librosa.display.waveplot(x)\n",
        "    plt.savefig('wavelets/train/'+cls+'/'+str(cnt)+'.png')\n",
        "    plt.close()\n",
        "  \n",
        "  cnt = 0\n",
        "  for nm in test_names:\n",
        "    cnt+=1\n",
        "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
        "    #plt.figure(figsize=(14, 5))\n",
        "    librosa.display.waveplot(x)\n",
        "    plt.savefig('wavelets/test/'+cls+'/'+str(cnt)+'.png')\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7aFO6Q4KOXi",
        "outputId": "ea634229-3cad-40b1-e6d7-ec97b12ac901"
      },
      "source": [
        "import os\n",
        "classes = [a for a in os.listdir('genres') if '.' not in a]\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKQS6NweDHi6"
      },
      "source": [
        "## Spectrogram generation.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnwuA7h19z48"
      },
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "\n",
        "def func1(cls):\n",
        "  img_names = os.listdir('genres/'+cls)\n",
        "  os.makedirs('spectrogram/train/'+cls)\n",
        "  os.makedirs('spectrogram/test/'+cls)\n",
        "  print(cls)\n",
        "  train_names = img_names[:60]\n",
        "  test_names = img_names[60:]\n",
        "  cnt = 0\n",
        "  for nm in train_names:\n",
        "    cnt+=1\n",
        "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
        "    X = librosa.stft(x)\n",
        "    Xdb = librosa.amplitude_to_db(abs(X))\n",
        "    librosa.display.specshow(Xdb)\n",
        "    plt.savefig('spectrogram/train/'+cls+'/'+str(cnt)+'.png')\n",
        "    plt.close()\n",
        "  \n",
        "  cnt = 0\n",
        "  for nm in test_names:\n",
        "    cnt+=1\n",
        "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
        "    X = librosa.stft(x)\n",
        "    Xdb = librosa.amplitude_to_db(abs(X))\n",
        "    librosa.display.specshow(Xdb)\n",
        "    plt.savefig('spectrogram/test/'+cls+'/'+str(cnt)+'.png')\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrtBWlF4_YXb",
        "outputId": "c8732337-479b-4e71-fa07-4132d24cefdb"
      },
      "source": [
        "import os\n",
        "classes = [a for a in os.listdir('genres') if '.' not in a]\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQy_bqnn9rmj"
      },
      "source": [
        "## Model training.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgrqLPZcK0MX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjWqYansStBI"
      },
      "source": [
        "labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "img_size = 256\n",
        "def get_data(data_dir):\n",
        "    data = [] \n",
        "    for label in labels: \n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.array(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZev-1aYS5tp"
      },
      "source": [
        "train = get_data('spectrogram/train')\n",
        "val = get_data('spectrogram/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOMMXUMnTNAt"
      },
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "for feature, label in train:\n",
        "  x_train.append(feature)\n",
        "  y_train.append(label)\n",
        "\n",
        "for feature, label in val:\n",
        "  x_val.append(feature)\n",
        "  y_val.append(label)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = np.array(x_train) / 255\n",
        "x_val = np.array(x_val) / 255\n",
        "\n",
        "x_train.reshape(-1, img_size, img_size, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_val.reshape(-1, img_size, img_size, 1)\n",
        "y_val = np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HMhdmkUTebr"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        #rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        #horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frApKmq_Tnkk"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(256,256,3)))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJFFmPkVTr-f"
      },
      "source": [
        "opt = Adam(lr=0.0001)\n",
        "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RciGH_6YTzlf"
      },
      "source": [
        "history = model.fit(x_train,y_train,epochs = 500, validation_data = (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYRkEJTjmAiF"
      },
      "source": [
        "import pickle\n",
        "model.save_weights('500_epoch_simple_lr.cpkt')\n",
        "\n",
        "pickle.dump(history.history, open('history_500_epoch_simple.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GJSYHmQT6EA"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(500)\n",
        "\n",
        "plt.figure(figsize=(25, 15))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "rSSZy4h5XwKg",
        "outputId": "4e8c12b5-5fb2-4d58-e512-a7ae663916d9"
      },
      "source": [
        "import pickle\n",
        "history = pickle.load(open('history_500_epoch_simple.pkl','rb'))\n",
        "acc = history['accuracy']\n",
        "val_acc = history['val_accuracy']\n",
        "loss = history['loss']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "epochs_range = range(500)\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
        "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=10)\n",
        "ax1.plot(epochs_range, acc, label='Training Accuracy', c = '#4CAF50', linewidth=4)\n",
        "ax1.plot(epochs_range, val_acc, label='Validation Accuracy', c='red', linewidth=4)\n",
        "ax1.legend()\n",
        "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
        "ax1.set_ylabel('Accuracy',fontsize=18)\n",
        "ax1.set_xlabel('Epoch',fontsize=18)\n",
        "\n",
        "ax2.plot(epochs_range, loss, label='Training Loss',c = '#4CAF50', linewidth=4)\n",
        "ax2.plot(epochs_range, val_loss, label='Validation Loss', c='red', linewidth=4)\n",
        "ax2.legend()\n",
        "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
        "ax2.set_ylabel('Loss',fontsize=18)\n",
        "ax2.set_xlabel('Epoch',fontsize=18)\n",
        "fig.tight_layout(pad=3.0)\n",
        "#plt.show()\n",
        "plt.savefig('sim_plot1.png',bbox_inches = 'tight')\n",
        "plt.clf()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLe4sDlebDiE"
      },
      "source": [
        "predictions = model.predict_classes(x_val)\n",
        "predictions = predictions.reshape(1,-1)[0]\n",
        "print(classification_report(y_val, predictions, target_names = labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbG03a-7Yayv"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "cm1 = confusion_matrix(y_val, predictions)\n",
        "df_cm = pd.DataFrame(cm1, index = [i for i in labels],\n",
        "              columns = [i for i in labels])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True,cmap=\"RdPu\")\n",
        "plt.savefig('confusion_mrtx1.png',bbox_inches = 'tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhAssCnpc-zC"
      },
      "source": [
        "# Transfer Learning based modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL_xNwxtbZGE"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape = (256, 256, 3), include_top = False, weights = \"imagenet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMP5zEkjboya"
      },
      "source": [
        "base_model.trainable = False\n",
        "model = tf.keras.Sequential([base_model,\n",
        "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
        "                                 tf.keras.layers.Dropout(0.2),\n",
        "                                 tf.keras.layers.Dense(10, activation=\"softmax\")                                     \n",
        "                                ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skCTJ9t8bzN5"
      },
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history1 = model.fit(x_train,y_train,epochs = 500 , validation_data = (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvaP0P-cD-Hv"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D30aRr_Lfc9M"
      },
      "source": [
        "acc = history1.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy']\n",
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "\n",
        "epochs_range = range(500)\n",
        "\n",
        "plt.figure(figsize=(25, 15))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-opt3HYbhRP"
      },
      "source": [
        "import pickle\n",
        "history = pickle.load(open('history_500_epoch_tr.pkl','rb'))\n",
        "acc = history['accuracy']\n",
        "val_acc = history['val_accuracy']\n",
        "loss = history['loss']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "epochs_range = range(500)\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
        "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=10)\n",
        "ax1.plot(epochs_range, acc, label='Training Accuracy', c = '#4CAF50', linewidth=4)\n",
        "ax1.plot(epochs_range, val_acc, label='Validation Accuracy', c='red', linewidth=4)\n",
        "ax1.legend()\n",
        "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
        "ax1.set_ylabel('Accuracy',fontsize=18)\n",
        "ax1.set_xlabel('Epoch',fontsize=18)\n",
        "\n",
        "ax2.plot(epochs_range, loss, label='Training Loss',c = '#4CAF50', linewidth=4)\n",
        "ax2.plot(epochs_range, val_loss, label='Validation Loss', c='red', linewidth=4)\n",
        "ax2.legend()\n",
        "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
        "ax2.set_ylabel('Loss',fontsize=18)\n",
        "ax2.set_xlabel('Epoch',fontsize=18)\n",
        "fig.tight_layout(pad=3.0)\n",
        "#plt.show()\n",
        "plt.savefig('tfr_plot1.png',bbox_inches = 'tight')\n",
        "plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf1JjtBAfrSi"
      },
      "source": [
        "predictions = model.predict_classes(x_val)\n",
        "predictions = predictions.reshape(1,-1)[0]\n",
        "print(classification_report(y_val, predictions, target_names = labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1UcfoXZcE7z"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "cm1 = confusion_matrix(y_val, predictions)\n",
        "df_cm = pd.DataFrame(cm1, index = [i for i in labels],\n",
        "              columns = [i for i in labels])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True,cmap=\"RdPu\")\n",
        "plt.savefig('confusion_mrtx2.png',bbox_inches = 'tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwzSfGdsshTN"
      },
      "source": [
        "model.save_weights('500_epoch_transfer_lr.cpkt')\n",
        "pickle.dump(history1.history, open('history_500_epoch_tr.pkl','wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkSwbJkkV5cp"
      },
      "source": [
        "## Multi-Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sRHHpGBdV8fQ"
      },
      "source": [
        "sp_train = get_data('spectrogram/train')\n",
        "sp_val = get_data('spectrogram/test')\n",
        "\n",
        "wv_train = get_data('wavelets/train')\n",
        "wv_val = get_data('wavelets/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh0O-VhDWHiX"
      },
      "source": [
        "x_sp_train = []\n",
        "y_sp_train = []\n",
        "x_sp_val = []\n",
        "y_sp_val = []\n",
        "\n",
        "for feature, label in sp_train:\n",
        "  x_sp_train.append(feature)\n",
        "  y_sp_train.append(label)\n",
        "\n",
        "for feature, label in sp_val:\n",
        "  x_sp_val.append(feature)\n",
        "  y_sp_val.append(label)\n",
        "\n",
        "# Normalize the data\n",
        "x_sp_train = np.array(x_sp_train) / 255\n",
        "x_sp_val = np.array(x_sp_val) / 255\n",
        "\n",
        "x_sp_train.reshape(-1, img_size, img_size, 1)\n",
        "y_sp_train = np.array(y_sp_train)\n",
        "\n",
        "x_sp_val.reshape(-1, img_size, img_size, 1)\n",
        "y_sp_val = np.array(y_sp_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWFkA_iJW7v_"
      },
      "source": [
        "x_wv_train = []\n",
        "y_wv_train = []\n",
        "x_wv_val = []\n",
        "y_wv_val = []\n",
        "\n",
        "for feature, label in wv_train:\n",
        "  x_wv_train.append(feature)\n",
        "  y_wv_train.append(label)\n",
        "\n",
        "for feature, label in wv_val:\n",
        "  x_wv_val.append(feature)\n",
        "  y_wv_val.append(label)\n",
        "\n",
        "# Normalize the data\n",
        "x_wv_train = np.array(x_wv_train) / 255\n",
        "x_wv_val = np.array(x_wv_val) / 255\n",
        "\n",
        "x_wv_train.reshape(-1, img_size, img_size, 1)\n",
        "y_wv_train = np.array(y_wv_train)\n",
        "\n",
        "x_wv_val.reshape(-1, img_size, img_size, 1)\n",
        "y_wv_val = np.array(y_wv_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQoJT8abXgqm"
      },
      "source": [
        "datagen_sp = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        #rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        #horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen_sp.fit(x_sp_train)\n",
        "\n",
        "datagen_wv = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        #rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        #horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen_wv.fit(x_wv_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAk1TubMZjJV"
      },
      "source": [
        "input_sp = keras.Input(shape=(256,256,3))\n",
        "input_wv = keras.Input(shape=(256,256,3))\n",
        "\n",
        "x = Conv2D(32,3,padding=\"same\", activation=\"relu\")(input_sp)\n",
        "x = MaxPool2D()(x)\n",
        "x = Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "x = MaxPool2D()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128,activation=\"relu\")(x)\n",
        "x = keras.Model(inputs=input_sp, outputs=x)\n",
        "\n",
        "y = Conv2D(32,3,padding=\"same\", activation=\"relu\")(input_wv)\n",
        "y = MaxPool2D()(y)\n",
        "y = Conv2D(64, 3, padding=\"same\", activation=\"relu\")(y)\n",
        "y = MaxPool2D()(y)\n",
        "y = Dropout(0.4)(y)\n",
        "y = Flatten()(y)\n",
        "y = Dense(128,activation=\"relu\")(y)\n",
        "y = keras.Model(inputs=input_wv, outputs=y)\n",
        "\n",
        "from tensorflow.keras.layers import concatenate\n",
        "combined = concatenate([x.output, y.output])\n",
        "\n",
        "z = Dense(32, activation=\"relu\")(combined)\n",
        "z = Dense(10, activation=\"softmax\")(z)\n",
        "\n",
        "model = keras.Model(inputs=[x.input, y.input], outputs=z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K25O827PcoKy"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LsfjV2ucprS"
      },
      "source": [
        "opt = Adam(lr=0.0001)\n",
        "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cPYQFBDc1KR"
      },
      "source": [
        "history2 = model.fit([x_sp_train,x_wv_train],y_sp_train,epochs = 500, validation_data = ([x_sp_val,x_wv_val], y_sp_val))\n",
        "import pickle\n",
        "model.save_weights('500_epoch_multi_lr.cpkt')\n",
        "pickle.dump(history2.history, open('history_500_epoch_multi.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5tiBlgtduBp"
      },
      "source": [
        "acc = history2.history['accuracy']\n",
        "val_acc = history2.history['val_accuracy']\n",
        "loss = history2.history['loss']\n",
        "val_loss = history2.history['val_loss']\n",
        "\n",
        "epochs_range = range(500)\n",
        "\n",
        "plt.figure(figsize=(25, 15))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMQeqjWgeIlw"
      },
      "source": [
        "import pickle\n",
        "history = pickle.load(open('history_500_epoch_multi.pkl','rb'))\n",
        "acc = history['accuracy']\n",
        "val_acc = history['val_accuracy']\n",
        "loss = history['loss']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "epochs_range = range(500)\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
        "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=10)\n",
        "ax1.plot(epochs_range, acc, label='Training Accuracy', c = '#4CAF50', linewidth=4)\n",
        "ax1.plot(epochs_range, val_acc, label='Validation Accuracy', c='red', linewidth=4)\n",
        "ax1.legend()\n",
        "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
        "ax1.set_ylabel('Accuracy',fontsize=18)\n",
        "ax1.set_xlabel('Epoch',fontsize=18)\n",
        "\n",
        "ax2.plot(epochs_range, loss, label='Training Loss',c = '#4CAF50', linewidth=4)\n",
        "ax2.plot(epochs_range, val_loss, label='Validation Loss', c='red', linewidth=4)\n",
        "ax2.legend()\n",
        "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
        "ax2.set_ylabel('Loss',fontsize=18)\n",
        "ax2.set_xlabel('Epoch',fontsize=18)\n",
        "fig.tight_layout(pad=3.0)\n",
        "#plt.show()\n",
        "plt.savefig('multi_plot1.png',bbox_inches = 'tight')\n",
        "plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-YbdgId-lQ"
      },
      "source": [
        "predictions = model.predict([x_sp_val,x_wv_val])\n",
        "print(classification_report(y_wv_val, np.argmax(predictions, axis=1), target_names = labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VyAzWhIemG_"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "cm1 = confusion_matrix(y_val, predictions)\n",
        "df_cm = pd.DataFrame(cm1, index = [i for i in labels],\n",
        "              columns = [i for i in labels])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True,cmap=\"RdPu\")\n",
        "plt.savefig('confusion_mrtx3.png',bbox_inches = 'tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4uBCt0dd36HJ",
        "outputId": "0ed11f0d-fa2a-4c14-b5c1-123ebf734a64"
      },
      "source": [
        "epochs_range = range(500)\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle \n",
        "history = pickle.load(open('history_500_epoch_simple.pkl','rb'))\n",
        "acc = history['accuracy']\n",
        "val_acc = history['val_accuracy']\n",
        "loss = history['loss']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "history = pickle.load(open('history_500_epoch_tr.pkl','rb'))\n",
        "acc1 = history['accuracy']\n",
        "val_acc1 = history['val_accuracy']\n",
        "loss1 = history['loss']\n",
        "val_loss1 = history['val_loss']\n",
        "\n",
        "history = pickle.load(open('history_500_epoch_multi.pkl','rb'))\n",
        "acc2 = history['accuracy']\n",
        "val_acc2 = history['val_accuracy']\n",
        "loss2 = history['loss']\n",
        "val_loss2 = history['val_loss']\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
        "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=10)\n",
        "ax1.plot(epochs_range, acc, label='CNN Training Accuracy', c = '#4CAF50', linewidth=4)\n",
        "ax1.plot(epochs_range, val_acc, label='CNN Validation Accuracy', c='red', linewidth=4)\n",
        "ax1.plot(epochs_range, acc1, label='Transfer learning Training Accuracy', c = '#e72866', linewidth=4)\n",
        "ax1.plot(epochs_range, val_acc1, label='Transfer learning Validation Accuracy', c='#282ec7', linewidth=4)\n",
        "ax1.plot(epochs_range, acc2, label='Multi modal Training Accuracy', c = '#171c1c', linewidth=4)\n",
        "ax1.plot(epochs_range, val_acc2, label='Multi modal Validation Accuracy', c='#62176e', linewidth=4)\n",
        "\n",
        "ax1.legend()\n",
        "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
        "ax1.set_ylabel('Accuracy',fontsize=18)\n",
        "ax1.set_xlabel('Epoch',fontsize=18)\n",
        "\n",
        "ax2.plot(epochs_range, loss, label='CNN Training Loss',c = '#4CAF50', linewidth=4)\n",
        "ax2.plot(epochs_range, val_loss, label='CNN Validation Loss', c='red', linewidth=4)\n",
        "ax2.plot(epochs_range, loss1, label='Transfer learning Training Loss',c = '#c72866', linewidth=4)\n",
        "ax2.plot(epochs_range, val_loss1, label='Transfer learning Validation Loss', c='#282ec7', linewidth=4)\n",
        "ax2.plot(epochs_range, loss2, label='Multi modal Training Loss',c = '#171c1c', linewidth=4)\n",
        "ax2.plot(epochs_range, val_loss2, label='Multi modal learning Validation Loss', c='#62176e', linewidth=4)\n",
        "\n",
        "ax2.legend()\n",
        "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
        "ax2.set_ylabel('Loss',fontsize=18)\n",
        "ax2.set_xlabel('Epoch',fontsize=18)\n",
        "fig.tight_layout(pad=3.0)\n",
        "#plt.show()\n",
        "plt.savefig('all_1.png',bbox_inches = 'tight')\n",
        "plt.clf()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvqSlPNI4C5b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}